{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkEy86lJ4Cub"
      },
      "source": [
        "# <a name=\"0\">Machine Learning Lab\n",
        "\n",
        "Build a classfier to predict the __label__ field (substitute or not substitute) of the product substitute dataset.\n",
        "\n",
        "### Final Project Problem: Product Substitute Prediction\n",
        "\n",
        "__Problem Definition__:\n",
        "Given a pair of products, (A, B), we say that B is a \"substitute\" for A if a customer would buy B in place of A -- say, if A were out of stock.\n",
        "\n",
        "The goal of this project is to predict a substitute relationship between pairs of products. Complete the tasks in this notebook and submit your notebook via Colab  \n",
        "\n",
        "1. <a href=\"#1\">Read the datasets</a> (Given)\n",
        "2. <a href=\"#2\">Data Processing</a> (Implement)\n",
        "    * <a href=\"#21\">Exploratory Data Analysis</a>\n",
        "    * <a href=\"#22\">Select features to build the model</a> (Suggested)\n",
        "    * <a href=\"#23\">Train - Validation - Test Datasets</a>\n",
        "    * <a href=\"#24\">Data Processing with Pipeline</a>\n",
        "3. <a href=\"#3\">Train (and Tune) a Classifier on the Training Dataset</a> (Implement)\n",
        "4. <a href=\"#3\">Make Predictions on the Test Dataset</a> (Implement)\n",
        "\n",
        "\n",
        "__Datasets and Files:__\n",
        "\n",
        "\n",
        "* __training.csv__: Training data with product pair features and corresponding labels:\n",
        "> - `ID:` ID of the record\n",
        "> - `label:` Tells whether the key and candidate products are substitutes (1) or not (0).\n",
        "> - `key_asin ...:` Key product ASIN features\n",
        "> - `cand_asin ...:` Candidate product ASIN features\n",
        "\n",
        "\n",
        "* __public_test_features.csv__: Test data with product pairs features __without__ labels:\n",
        "> - `ID:` ID of the record\n",
        "> - `key_asin ...:` Key product ASIN features\n",
        "> - `cand_asin ...:` Candidate product ASIN features\n",
        "\n",
        "\n",
        "* __metadata-dataset.xlsx__: Provides detailed information about all key_ and cand_ columns in the training and test sets. Try to select some useful features to include in the model, as not all of them are suitable. `|Region Id|MarketPlace Id|ASIN|Binding Code|binding_description|brand_code|case_pack_quantity|, ...`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubosr2Nq4Cuf"
      },
      "source": [
        "## 1. <a name=\"1\">Read the datasets</a> (Given)\n",
        "(<a href=\"#0\">Go to top</a>)\n",
        "</br>\n",
        "<a href=\"https://propensity-labs-screening.s3.amazonaws.com/machine_learning/ml_data.zip\">Download Dataset</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCyv5Vff4Cuf"
      },
      "source": [
        "Then, we read the __training__ and __test__ datasets into dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "NHrP_vOn4Cuf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59081099-c1bb-46ac-ac2d-b48867217304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-62f7d8dbf4a0>:9: DtypeWarning: Columns (34,139,154,157,212) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(data_path)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Load the dataset\n",
        "data_path = '/content/drive/MyDrive/Karini_Ai_test/ml_data/ml_data/training.csv'  # Replace with the path to your dataset\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NukXQipp4Cul"
      },
      "source": [
        "## 2. <a name=\"2\">Data Processing</a> (Implement)\n",
        "(<a href=\"#0\">Go to top</a>)\n",
        "\n",
        "### 2.1 <a name=\"21\">Exploratory Data Analysis</a>\n",
        "\n",
        "We look at number of rows, columns, and some simple statistics of the datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "5AETRVq44Cum"
      },
      "outputs": [],
      "source": [
        "# Implement EDA here\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = df.drop(columns=['label'])  # Features\n",
        "y = df['label']  # Target variable\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23T5pcGb4Cun"
      },
      "source": [
        "### 2.2 <a name=\"22\">Select features to build the model</a>\n",
        "\n",
        "For a quick start, we recommend using only a few of the numerical features for both key_ and cand_ ASINs: __item_package_quantity__, __item_height__, __item_width__, __item_length__, __item_weight__, __pkg_height__, __pkg_width__, __pkg_length__, __pkg_weight__. Feel free to explore other fields from the metadata-dataset.xlsx file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "7_ICst2B4Cun",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "42199a2c-2e16-4488-d624-c125bb4a2c5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-52-aa351cdca6c0>:10: DtypeWarning: Columns (34,139,154,157,212) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(data_path)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Shape of passed values is (36803, 77), indices imply (36803, 101)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-aa351cdca6c0>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mX_numeric_imputed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimputer_numeric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumeric_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Combine imputed numerical features with remaining features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 )\n\u001b[1;32m    721\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m                 mgr = ndarray_to_mgr(\n\u001b[0m\u001b[1;32m    723\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                     \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    347\u001b[0m     )\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m     \u001b[0m_check_values_indices_shape_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mpassed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mimplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of passed values is {passed}, indices imply {implied}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (36803, 77), indices imply (36803, 101)"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the dataset\n",
        "data_path = r'/content/drive/MyDrive/Karini_Ai_test/ml_data/ml_data/training.csv'  # Replace with the path to your dataset\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = df.drop(columns=['label'])  # Features\n",
        "y = df['label']  # Target variable\n",
        "\n",
        "# Handle missing values for numerical features with SimpleImputer\n",
        "numeric_features = X.select_dtypes(include=np.number).columns\n",
        "imputer_numeric = SimpleImputer(strategy='mean')\n",
        "\n",
        "X_numeric_imputed_values = imputer_numeric.fit_transform(X[numeric_features])\n",
        "\n",
        "\n",
        "X_numeric_imputed = pd.DataFrame(imputer_numeric.fit_transform(X[numeric_features]), columns=numeric_features)\n",
        "\n",
        "# Combine imputed numerical features with remaining features\n",
        "X_remaining = X.drop(columns=numeric_features)\n",
        "X_final = pd.concat([X_numeric_imputed, X_remaining], axis=1)\n",
        "\n",
        "# Calculate correlation matrix\n",
        "corr_matrix = X_final.corr()\n",
        "\n",
        "# Plot correlation matrix heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "# SelectKBest feature selection\n",
        "selector = SelectKBest(score_func=f_classif, k=10)  # Choose the number of top features to select (e.g., k=10)\n",
        "X_selected = selector.fit_transform(X_final, y)\n",
        "selected_features = X_final.columns[selector.get_support()]\n",
        "\n",
        "# Plot the scores of features\n",
        "plt.figure(figsize=(10, 6))\n",
        "scores = -np.log10(selector.pvalues_)\n",
        "plt.bar(range(len(X_final.columns)), scores)\n",
        "plt.xticks(range(len(X_final.columns)), X_final.columns, rotation='vertical')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('-log(p-value)')\n",
        "plt.title('Feature Importance Scores')\n",
        "plt.show()\n",
        "\n",
        "# Print selected features\n",
        "print(\"Selected Features:\", selected_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAod3ayh4Cuo"
      },
      "source": [
        "### 2.3 <a name=\"23\">Train - Validation Datasets</a>\n",
        "(<a href=\"#2\">Go to Data Processing</a>)\n",
        "\n",
        "We already have training and test datasets, however the test dataset is missing the labels - the goal of the project is to predict these labels.\n",
        "\n",
        "To produce a validation set to evaluate model performance before submitting  split the training dataset into train and validation. Validation data you get here will be used later in section 3 to tune your classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "3V5aj4mW4Cuo"
      },
      "outputs": [],
      "source": [
        "# Implement here\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Assuming X contains your feature matrix with both numeric and categorical features\n",
        "# Assuming 'categorical_features' contains the list of categorical feature names\n",
        "# Convert float values to strings within categorical features\n",
        "X[categorical_features] = X[categorical_features].astype(str)\n",
        "\n",
        "# Separate categorical and numeric features\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "X[categorical_features] = X[categorical_features].astype(str)\n",
        "numeric_features = X.select_dtypes(exclude=['object']).columns\n",
        "\n",
        "\n",
        "\n",
        "# Now X_processed contains the preprocessed feature matrix with all features in numeric format\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xW_8mZ4i4Cur"
      },
      "source": [
        "### 2.4 <a name=\"24\">Data processing with Pipeline</a>\n",
        "\n",
        "Build a [pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)  to impute the missing values and scale the numerical features, and finally train the classifier on the imputed and scaled dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "-faYi9dK4Cut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dc508ab-5489-4740-cc8e-df2e98ab4646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No categorical features found.\n"
          ]
        }
      ],
      "source": [
        "# Implement here\n",
        "#One-hot encode categorical features\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "X_categorical_encoded = encoder.fit_transform(X[categorical_features])\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Check if there are categorical features before one-hot encoding\n",
        "if len(categorical_features) > 0:\n",
        "    # One-hot encode categorical features\n",
        "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "    X_categorical_encoded = encoder.fit_transform(X[categorical_features])\n",
        "\n",
        "    # Check if there are features after one-hot encoding\n",
        "    if X_categorical_encoded.shape[1] > 0:\n",
        "        # Reduce dimensionality of the sparse matrix\n",
        "        svd = TruncatedSVD(n_components=100)  # Adjust the number of components as needed\n",
        "        X_reduced = svd.fit_transform(X_categorical_encoded)\n",
        "    else:\n",
        "        print(\"No features remaining after one-hot encoding.\")\n",
        "else:\n",
        "    print(\"No categorical features found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ49qrqB4Cut"
      },
      "source": [
        "## 3. <a name=\"3\">Train (and Tune) a Classifier</a> (Implement)\n",
        "(<a href=\"#0\">Go to top</a>)\n",
        "\n",
        "Train and tune the classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bzziwob54Cuu"
      },
      "outputs": [],
      "source": [
        "# Implement here\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load training and testing datasets\n",
        "train_file = r'C:\\Users\\tejas\\Downloads\\ml_data\\training.csv'\n",
        "test_file = r'C:\\Users\\tejas\\Downloads\\ml_data\\public_test_features.csv'\n",
        "\n",
        "# # Step 1: Load the training and testing data\n",
        "# train_data = pd.read_csv(\"training.csv\")\n",
        "# test_data = pd.read_csv(\"public_test_features.csv\")\n",
        "\n",
        "# Load training and testing datasets with explicit data types or low_memory=False\n",
        "# train_file = pd.read_csv(r'C:\\Users\\tejas\\Downloads\\ml_data\\training.csv', low_memory=False)\n",
        "# test_file = pd.read_csv(r'C:\\Users\\tejas\\Downloads\\ml_data\\public_test_features.csv', low_memory=False)\n",
        "\n",
        "# Continue with your preprocessing and modeling code as before\n",
        "\n",
        "\n",
        "# Define chunk size for incremental loading\n",
        "chunk_size = 10000  # Adjust the chunk size as needed based on your available memory\n",
        "\n",
        "# Initialize empty lists to store chunk-wise processed data\n",
        "train_chunks = []\n",
        "test_chunks = []\n",
        "\n",
        "# Iterate over chunks of training data\n",
        "for chunk in pd.read_csv(train_file, chunksize=chunk_size, low_memory=False):\n",
        "    # Preprocess each chunk as needed (e.g., fill missing values, encode categorical variables)\n",
        "    chunk.fillna(chunk.mean(), inplace=True)  # Example: fill missing values with mean\n",
        "    # Append preprocessed chunk to the list\n",
        "    train_chunks.append(chunk)\n",
        "\n",
        "# Concatenate all preprocessed training data chunks into a single dataframe\n",
        "train_data = pd.concat(train_chunks, ignore_index=True)\n",
        "\n",
        "# Iterate over chunks of testing data\n",
        "for chunk in pd.read_csv(test_file, chunksize=chunk_size, low_memory=False):\n",
        "    # Preprocess each chunk as needed (similar to training data)\n",
        "    chunk.fillna(chunk.mean(), inplace=True)  # Example: fill missing values with mean\n",
        "    # Append preprocessed chunk to the list\n",
        "    test_chunks.append(chunk)\n",
        "\n",
        "# Concatenate all preprocessed testing data chunks into a single dataframe\n",
        "test_data = pd.concat(test_chunks, ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step 2: Separate features and target variable in the training data\n",
        "X_train = train_data.drop(columns=['label'])\n",
        "y_train = train_data['label']\n",
        "\n",
        "# Drop non-numeric columns from the feature matrix\n",
        "X_train_numeric = X_train.select_dtypes(include=['number'])\n",
        "\n",
        "\n",
        "\n",
        "# Step 3: Preprocess the training data\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train_numeric)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "\n",
        "# Step 4: Train a predictive model\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: Preprocess the testing data\n",
        "# Check for missing columns in testing data compared to training data\n",
        "missing_columns = set(X_train.columns) - set(X_test.columns)\n",
        "\n",
        "# # If there are missing columns, add them to testing data and fill with zeros\n",
        "# if missing_columns:\n",
        "#     for col in missing_columns:\n",
        "#         X_test[col] = 0  # Fill missing columns with zeros\n",
        "\n",
        "# # Check for additional columns in testing data compared to training data\n",
        "# additional_columns = set(X_test.columns) - set(X_train.columns)\n",
        "\n",
        "# If there are additional columns, remove them from testing data\n",
        "if additional_columns:\n",
        "    X_test = X_test.drop(columns=additional_columns)\n",
        "\n",
        "# Now proceed with imputing missing values and scaling as before\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "X_test_scaled = scaler.transform(X_test_imputed)\n",
        "\n",
        "\n",
        "# Handle missing values (use the same imputer as in training)\n",
        "# X_test = test_data.drop(columns=['label'])\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Scale features (use the same scaler as in training)\n",
        "X_test_scaled = scaler.transform(X_test_imputed)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZpmxtihZEFf",
        "outputId": "6698cf7c-090b-49be-b357-5a8f18fcbc88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl2smdli4Cuu"
      },
      "source": [
        "## 4. <a name=\"4\">Make Predictions on the Test Dataset</a> (Implement)\n",
        "(<a href=\"#0\">Go to top</a>)\n",
        "\n",
        "Use the trained classifier to predict the labels on the test set. Test accuracy would be displayed upon a valid submission to the leaderboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt4JEQoC4Cuu"
      },
      "outputs": [],
      "source": [
        "# Implement here\n",
        "\n",
        "# Get test data to test the classifier\n",
        "# ! test data should come from public_test_features.csv !\n",
        "# ...\n",
        "\n",
        "# Use the trained model to make predictions on the test dataset\n",
        "# test_predictions = ...\n",
        "\n",
        "\n",
        "# Step 6: Predict labels for the testing data\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Step 7: Output the predicted labels\n",
        "output = pd.DataFrame({'ID': test_data['ID'], 'Predicted_Label': y_pred})\n",
        "output.to_csv('predicted_labels.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "conda_pytorch_p39",
      "language": "python",
      "name": "conda_pytorch_p39"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}